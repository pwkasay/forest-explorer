# ğŸ„ Forest Carbon Data Explorer

**A full-stack data engineering demo ingesting USFS Forest Inventory & Analysis data through automated QA/QC pipelines, transforming it with dbt, and serving it via a Vue dashboard with interactive geospatial carbon visualizations.**

Built as a portfolio project demonstrating data engineering skills for climate tech â€” a data stack that bridges genomic sequencing, geospatial analysis, and carbon accounting.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vue 3 Frontend (Vite + Leaflet + Chart.js)             â”‚
â”‚  Interactive map, carbon dashboards, QA/QC reports      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FastAPI Backend (Python 3.13)                          â”‚
â”‚  REST API, data ingestion, validation pipelines         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  dbt Transformation Layer                               â”‚
â”‚  staging â†’ marts: clean, versioned, documented models   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL + PostGIS                                   â”‚
â”‚  Spatial queries, forest plot data, carbon estimates     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## What This Demonstrates

| Requirement | How This Project Shows It |
|---|---|
| SQL + dbt transformation layers | dbt models transforming raw FIA data into carbon-ready marts |
| Internal data apps (Streamlit or similar) | Vue dashboard with carbon metrics, maps, QA/QC visibility |
| Geospatial workflows (vector/raster QA/QC) | PostGIS spatial queries, boundary validation, Leaflet maps |
| Automated QA/QC pipelines | Validation framework catching data inconsistencies pre-delivery |
| Containerized connectors pulling from APIs | Dockerized FIA API ingestion pipeline |
| Cloud-native deployment (Docker, AWS/GCP) | Docker Compose stack, production-ready Dockerfile |
| Clean Python for data manipulation | Type-hinted, async Python 3.13 throughout |
| Treat internal teams as customers | Dashboard designed for non-technical Growth/Land team users |

## Tech Stack

**Backend:** Python 3.13, FastAPI, SQLAlchemy + GeoAlchemy2, asyncpg
**Data:** dbt-core + dbt-postgres, FIA API ingestion  
**Frontend:** Vue 3 (Composition API), Vite, Leaflet, Chart.js  
**Infrastructure:** PostgreSQL 16 + PostGIS 3.4, Docker Compose
**Geospatial:** PostGIS, Shapely, GeoPandas, pyproj

## Quick Start

```bash
# Clone and start everything
git clone https://github.com/pwkasay/forest-explorer.git
cd forest-explorer
cp .env.example .env
docker compose up -d

# Ingest FIA data for North Carolina (~5-15 min, downloads ~150MB from USFS)
docker compose exec backend python -m app.ingestion.fia_loader --state NC

# Load species reference and build dbt models
docker compose exec backend dbt seed --project-dir /app/dbt --profiles-dir /app/dbt
docker compose exec backend dbt run  --project-dir /app/dbt --profiles-dir /app/dbt
docker compose exec backend dbt test --project-dir /app/dbt --profiles-dir /app/dbt

# Open the dashboard
open http://localhost:3001          # Dashboard
# API docs at http://localhost:8002/docs
```

Or use the **Makefile** shortcuts:

```bash
make setup                          # docker compose up -d
make ingest                         # Ingest NC data (STATE=NC by default)
make ingest STATE=SC                # Ingest a different state
make dbt-seed                       # Load species reference CSV (run before dbt)
make dbt                            # Build staging views + mart tables (requires dbt-seed)
make dbt-test                       # Run 28 data quality tests
make test                           # Run backend unit tests (23 tests)
make lint                           # Ruff lint + format check
```

### Optional: County Boundaries & Climate Data

The core dashboard works with just FIA data, but two additional ingestion pipelines unlock county-level and climate endpoints:

```bash
make ingest-counties               # Census TIGER county polygons (~30s)
make ingest-climate                # PRISM 30-year climate normals (~3 min)
make dbt                           # Rebuild models with new data
```

These are **optional** â€” skip them to get up and running faster, or run them later to enable the `/api/v1/counties/` and `/api/v1/climate/` API endpoints.

### Data ingestion options

The ingestion pipeline supports all 50 US states. Each state downloads three CSV files (PLOT, TREE, COND) from the USFS FIA DataMart:

```bash
# Ingest a single state
docker compose exec backend python -m app.ingestion.fia_loader --state GA

# Ingest specific tables only (useful for debugging)
docker compose exec backend python -m app.ingestion.fia_loader --state NC --tables PLOT,TREE
```

State codes are standard two-letter abbreviations (NC, SC, GA, VA, FL, etc.). FIPS codes used in the API: NC=37, SC=45, GA=13.

## Project Structure

```
forest-explorer/
â”œâ”€â”€ .github/workflows/      # CI pipeline (lint + test on PRs)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # FastAPI route handlers
â”‚   â”‚   â”œâ”€â”€ core/           # Config, database, dependencies
â”‚   â”‚   â”œâ”€â”€ models/         # SQLAlchemy + GeoAlchemy2 ORM models
â”‚   â”‚   â”œâ”€â”€ schemas/        # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic, QA/QC engine
â”‚   â”‚   â””â”€â”€ ingestion/      # FIA, TIGER, and PRISM data loaders
â”‚   â”œâ”€â”€ dbt/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/    # 1:1 with raw tables, light cleaning
â”‚   â”‚   â”‚   â””â”€â”€ marts/      # Business-ready aggregations
â”‚   â”‚   â”œâ”€â”€ seeds/          # Reference data (species codes, etc.)
â”‚   â”‚   â””â”€â”€ macros/         # Reusable SQL snippets
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # Reusable Vue components
â”‚   â”‚   â”œâ”€â”€ views/          # Page-level views
â”‚   â”‚   â”œâ”€â”€ composables/    # Vue composition functions
â”‚   â”‚   â””â”€â”€ stores/         # Pinia state management
â”‚   â””â”€â”€ public/
â”œâ”€â”€ docker/                 # Dockerfiles
â”œâ”€â”€ scripts/                # Audit & dev utilities
â””â”€â”€ docker-compose.yml
```

## API Endpoints

Interactive docs at `http://localhost:8002/docs`.

| Method | Path | Description | Requires |
|--------|------|-------------|----------|
| GET | `/api/v1/carbon/summary/{statecd}` | Aggregate carbon stats | FIA data |
| GET | `/api/v1/carbon/species/{statecd}` | Top species by carbon density | FIA data |
| GET | `/api/v1/plots/{statecd}/geojson` | Plot locations as GeoJSON | FIA data |
| GET | `/api/v1/counties/{statecd}/geojson` | County boundary polygons | TIGER data (optional) |
| GET | `/api/v1/climate/{statecd}` | Plot-level climate metrics | PRISM data (optional) |
| POST | `/api/v1/qa/run` | Run QA/QC validation checks | FIA data |
| GET | `/api/v1/health/data` | Pipeline health status | â€” |
| POST | `/api/v1/ingest/{state_abbr}` | Trigger FIA ingestion | â€” |

State codes in paths are FIPS codes: NC=37, SC=45, GA=13.

## Dashboard Pages

- **Dashboard** (`/`) â€” KPI cards + species carbon chart
- **Map** (`/map`) â€” Leaflet map with plot clusters, carbon filters, species filter
- **QA/QC** (`/qa`) â€” Data validation results with severity badges

## Development

```bash
# Backend only (requires local PostgreSQL+PostGIS)
cd backend
python -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"
uvicorn app.main:app --reload

# Frontend only
cd frontend
npm install
npm run dev

# Run dbt
cd backend/dbt
dbt run && dbt test
```

## CI & Verification

GitHub Actions runs linting (`ruff`) and tests (`pytest`, `npm run build`) on every push and PR to `main`.

To verify the full pipeline locally after ingestion:

```bash
python scripts/audit_endpoints.py              # Hits every API endpoint, reports PASS/WARN/FAIL
python scripts/audit_endpoints.py --state 45   # Audit a specific state (FIPS code)
```

## Data Sources

- **USFS FIA Database API** â€” Tree-level measurements, carbon estimates, plot locations
- **Census TIGER/Line** â€” County boundary polygons (optional, enables `/api/v1/counties/` endpoint)
- **PRISM Climate Normals** â€” 30-year temperature/precipitation normals at 4km resolution (optional, enables `/api/v1/climate/` endpoint)
- **FIA Species Reference** â€” Tree species codes and common names (dbt seed)

## Key Concepts

This project works with **Forest Inventory & Analysis (FIA)** data from the USDA Forest Service â€” the same ground-truth dataset that underpins US forest carbon accounting. Key fields:

- `CARBON_AG` / `CARBON_BG` â€” Above/belowground carbon per tree (lbs)
- `DRYBIO_AG` / `DRYBIO_BG` â€” Dry biomass estimates
- `DIA` â€” Diameter at breast height (inches)
- `SPCD` â€” Species code (e.g., 131 = loblolly pine)
- `TPA_UNADJ` â€” Trees-per-acre expansion factor
- `FORTYPCD` â€” Forest type code
- `STDAGE` â€” Stand age (years)
- **Coordinate fuzzing** â€” Public FIA plot coordinates are displaced ~0.5â€“1 mile by USFS for privacy; carbon estimates remain precise but mapped locations are approximate

The dbt models transform these raw measurements into carbon-per-acre estimates by species, forest type, and geography

## License

MIT
